#![recursion_limit = "512"]

use std::path::Path;

use image::{DynamicImage, ImageBuffer};
use yolox_burn::model::{boxes::nms, weights, yolox::Yolox, BoundingBox};

use burn::{
    backend::{NdArray, Wgpu, Cuda},
    tensor::{backend::Backend, Device, Element, Tensor, TensorData},
};

const HEIGHT: usize = 640;
const WIDTH: usize = 640;

fn to_tensor<B: Backend, T: Element>(
    data: Vec<T>,
    shape: [usize; 3],
    device: &Device<B>,
) -> Tensor<B, 3> {
    Tensor::<B, 3>::from_data(
        TensorData::new(data, shape).convert::<B::FloatElem>(),
        device,
    )
    // [H, W, C] -> [C, H, W]
    .permute([2, 0, 1])
}

/// Draws bounding boxes on the given image.
///
/// # Arguments
///
/// * `image`: Original input image.
/// * `boxes` - Bounding boxes, grouped per class.
/// * `color` - [R, G, B] color values to draw the boxes.
/// * `ratio` - [x, y] aspect ratio to scale the predicted boxes.
///
/// # Returns
///
/// The image annotated with bounding boxes.
fn draw_boxes(
    image: DynamicImage,
    boxes: &[Vec<BoundingBox>],
    color: &[u8; 3],
    ratio: &[f32; 2], // (x, y) ratio
) -> DynamicImage {
    // Assumes x1 <= x2 and y1 <= y2
    fn draw_rect(
        image: &mut ImageBuffer<image::Rgb<u8>, Vec<u8>>,
        x1: u32,
        x2: u32,
        y1: u32,
        y2: u32,
        color: &[u8; 3],
    ) {
        for x in x1..=x2 {
            let pixel = image.get_pixel_mut(x, y1);
            *pixel = image::Rgb(*color);
            let pixel = image.get_pixel_mut(x, y2);
            *pixel = image::Rgb(*color);
        }
        for y in y1..=y2 {
            let pixel = image.get_pixel_mut(x1, y);
            *pixel = image::Rgb(*color);
            let pixel = image.get_pixel_mut(x2, y);
            *pixel = image::Rgb(*color);
        }
    }

    // Annotate the original image and print boxes information.
    let (image_h, image_w) = (image.height(), image.width());
    let mut image = image.to_rgb8();
    for (class_index, bboxes_for_class) in boxes.iter().enumerate() {
        for b in bboxes_for_class.iter() {
            let xmin = (b.xmin * ratio[0]).clamp(0., image_w as f32 - 1.);
            let ymin = (b.ymin * ratio[1]).clamp(0., image_h as f32 - 1.);
            let xmax = (b.xmax * ratio[0]).clamp(0., image_w as f32 - 1.);
            let ymax = (b.ymax * ratio[1]).clamp(0., image_h as f32 - 1.);

            println!(
                "Predicted {} ({:.2}) at [{:.2}, {:.2}, {:.2}, {:.2}]",
                class_index, b.confidence, xmin, ymin, xmax, ymax,
            );

            draw_rect(
                &mut image,
                xmin as u32,
                xmax as u32,
                ymin as u32,
                ymax as u32,
                color,
            );
        }
    }
    DynamicImage::ImageRgb8(image)
}

fn run_inference_gpu() -> Result<(), Box<dyn std::error::Error>> {
    println!("YOLO Infer with GPU (WGPU - Most Compatible)");
    let img_path = "/home/asus/zzzzz/hackathon/sdv_hackathon2/rt-rk/rust_ws/ucar_ai/sample/dog_bike_man.jpg";

    // Use WGPU backend (most compatible with YOLOX)
    let device = Device::<Cuda>::default();
    println!("WGPU device created successfully");
    
    // Create YOLOX-Tiny with WGPU backend
    let model: Yolox<Cuda> = Yolox::yolox_tiny_pretrained(weights::YoloxTiny::Coco, &device)
        .map_err(|err| format!("Failed to load pre-trained weights.\nError: {err}"))?;
    
    println!("Model loaded successfully on GPU (WGPU)");
    println!("Model variant: YOLOX-Tiny");
    
    run_benchmark_gpu(model, device, img_path, 5)
}

fn run_benchmark_gpu(
    model: Yolox<Cuda>,
    device: Device<Cuda>,
    img_path: &str,
    num_runs: usize,
) -> Result<(), Box<dyn std::error::Error>> {
    // Load and prepare image once
    let img = image::open(&img_path)
        .map_err(|err| format!("Failed to load image {img_path}.\nError: {err}"))?;

    println!("Image loaded: {}x{}", img.width(), img.height());

    let resized_img = img.resize_exact(
        WIDTH as u32,
        HEIGHT as u32,
        image::imageops::FilterType::Triangle,
    );

    let x = to_tensor(
        resized_img.into_rgb8().into_raw(),
        [HEIGHT, WIDTH, 3],
        &device,
    )
    .unsqueeze::<4>();

    println!("Running {} inference iterations...", num_runs);
    
    let mut inference_times = Vec::new();
    let mut total_boxes = Vec::new();

    for i in 1..=num_runs {
        println!("  Run {}/{}", i, num_runs);
        
        let start_time = std::time::Instant::now();
        let out = model.forward(x.clone());
        let inference_time = start_time.elapsed();
        
        // Post-processing for final run only
        let [_, num_boxes, num_outputs] = out.dims();
        let boxes = out.clone().slice([0..1, 0..num_boxes, 0..4]);
        let obj_scores = out.clone().slice([0..1, 0..num_boxes, 4..5]);
        let cls_scores = out.slice([0..1, 0..num_boxes, 5..num_outputs]);
        let scores = cls_scores * obj_scores;
        let boxes = nms(boxes, scores, 0.65, 0.5);
        
        inference_times.push(inference_time.as_millis() as f64);
        total_boxes.push(boxes);
        
        println!("    Inference time: {:.2}ms", inference_time.as_millis());
    }

    let avg_time = inference_times.iter().sum::<f64>() / num_runs as f64;
    let min_time = inference_times.iter().fold(f64::INFINITY, |a, &b| a.min(b));
    let max_time = inference_times.iter().fold(0.0f64, |a, &b| a.max(b));

    println!("\n=== GPU (WGPU) Inference Statistics ===");
    println!("Average inference time: {:.2}ms", avg_time);
    println!("Minimum inference time: {:.2}ms", min_time);
    println!("Maximum inference time: {:.2}ms", max_time);

    // Save output from the last run
    let (h, w) = (img.height(), img.width());
    let img_out = draw_boxes(
        img,
        &total_boxes[num_runs - 1][0],
        &[239u8, 62u8, 5u8],
        &[w as f32 / WIDTH as f32, h as f32 / HEIGHT as f32],
    );

    let img_path_obj = Path::new(&img_path);
    let output_path = img_path_obj.with_file_name("dog_bike_man.output_cuda.png");
    img_out.save(&output_path)?;
    println!("Output saved to: {:?}", output_path);
    
    Ok(())
}

fn run_inference_cpu() -> Result<(), Box<dyn std::error::Error>> {
    println!("YOLO Infer with CPU (NdArray)");
    let img_path = "/home/asus/zzzzz/hackathon/sdv_hackathon2/rt-rk/rust_ws/ucar_ai/sample/dog_bike_man.jpg";

    // Create CPU device
    let device = Device::<NdArray>::default();
    
    // Create YOLOX-Tiny with CPU backend
    let model: Yolox<NdArray> = Yolox::yolox_tiny_pretrained(weights::YoloxTiny::Coco, &device)
        .map_err(|err| format!("Failed to load pre-trained weights.\nError: {err}"))?;
    
    println!("Model loaded successfully on CPU");
    println!("Model variant: YOLOX-Tiny (should be the smallest/fastest variant)");
    
    run_benchmark_cpu(model, device, img_path, 5)
}

fn run_benchmark_cpu(
    model: Yolox<NdArray>,
    device: Device<NdArray>,
    img_path: &str,
    num_runs: usize,
) -> Result<(), Box<dyn std::error::Error>> {
    // Load and prepare image once
    let img = image::open(&img_path)
        .map_err(|err| format!("Failed to load image {img_path}.\nError: {err}"))?;

    println!("Image loaded: {}x{}", img.width(), img.height());

    let resized_img = img.resize_exact(
        WIDTH as u32,
        HEIGHT as u32,
        image::imageops::FilterType::Triangle,
    );

    let x = to_tensor(
        resized_img.into_rgb8().into_raw(),
        [HEIGHT, WIDTH, 3],
        &device,
    )
    .unsqueeze::<4>();

    println!("Running {} inference iterations...", num_runs);
    
    let mut inference_times = Vec::new();
    let mut total_boxes = Vec::new();

    for i in 1..=num_runs {
        println!("  Run {}/{}", i, num_runs);
        
        let start_time = std::time::Instant::now();
        let out = model.forward(x.clone());
        let inference_time = start_time.elapsed();
        
        // Post-processing for final run only
        let [_, num_boxes, num_outputs] = out.dims();
        let boxes = out.clone().slice([0..1, 0..num_boxes, 0..4]);
        let obj_scores = out.clone().slice([0..1, 0..num_boxes, 4..5]);
        let cls_scores = out.slice([0..1, 0..num_boxes, 5..num_outputs]);
        let scores = cls_scores * obj_scores;
        let boxes = nms(boxes, scores, 0.65, 0.5);
        
        inference_times.push(inference_time.as_millis() as f64);
        total_boxes.push(boxes);
        
        println!("    Inference time: {:.2}ms", inference_time.as_millis());
    }

    let avg_time = inference_times.iter().sum::<f64>() / num_runs as f64;
    let min_time = inference_times.iter().fold(f64::INFINITY, |a, &b| a.min(b));
    let max_time = inference_times.iter().fold(0.0f64, |a, &b| a.max(b));

    println!("\n=== CPU Inference Statistics ===");
    println!("Average inference time: {:.2}ms", avg_time);
    println!("Minimum inference time: {:.2}ms", min_time);
    println!("Maximum inference time: {:.2}ms", max_time);

    // Save output from the last run
    let (h, w) = (img.height(), img.width());
    let img_out = draw_boxes(
        img,
        &total_boxes[num_runs - 1][0],
        &[239u8, 62u8, 5u8],
        &[w as f32 / WIDTH as f32, h as f32 / HEIGHT as f32],
    );

    let img_path_obj = Path::new(&img_path);
    let output_path = img_path_obj.with_file_name("dog_bike_man.output_cpu.png");
    img_out.save(&output_path)?;
    println!("Output saved to: {:?}", output_path);
    
    Ok(())
}



fn run_nano_comparison() -> Result<(), Box<dyn std::error::Error>> {
    println!("=== YOLOX Model Size Comparison ===");
    
    let img_path = "/home/asus/zzzzz/hackathon/sdv_hackathon2/rt-rk/rust_ws/ucar_ai/sample/dog_bike_man.jpg";
    let device = Device::<NdArray>::default();
    
    // Test YOLOX-Nano (smallest)
    println!("\n1. Testing YOLOX-Nano (smallest model):");
    let nano_model: Yolox<NdArray> = Yolox::yolox_nano_pretrained(weights::YoloxNano::Coco, &device)
        .map_err(|err| format!("Failed to load YOLOX-Nano weights.\nError: {err}"))?;
    
    let img = image::open(&img_path)?;
    let resized_img = img.resize_exact(WIDTH as u32, HEIGHT as u32, image::imageops::FilterType::Triangle);
    let x = to_tensor(resized_img.into_rgb8().into_raw(), [HEIGHT, WIDTH, 3], &device).unsqueeze::<4>();
    
    let start = std::time::Instant::now();
    let _out = nano_model.forward(x.clone());
    let nano_time = start.elapsed();
    println!("  YOLOX-Nano inference: {:.2}ms", nano_time.as_millis());
    
    // Test YOLOX-Tiny (current)
    println!("\n2. Testing YOLOX-Tiny (current model):");
    let tiny_model: Yolox<NdArray> = Yolox::yolox_tiny_pretrained(weights::YoloxTiny::Coco, &device)
        .map_err(|err| format!("Failed to load YOLOX-Tiny weights.\nError: {err}"))?;
    
    let start = std::time::Instant::now();
    let _out = tiny_model.forward(x.clone());
    let tiny_time = start.elapsed();
    println!("  YOLOX-Tiny inference: {:.2}ms", tiny_time.as_millis());
    
    println!("\n=== Model Comparison Results ===");
    println!("YOLOX-Nano: {:.2}ms", nano_time.as_millis());
    println!("YOLOX-Tiny: {:.2}ms", tiny_time.as_millis());
    println!("Speedup with Nano: {:.2}x", tiny_time.as_secs_f64() / nano_time.as_secs_f64());
    
    Ok(())
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // First, let's compare model sizes
    match run_nano_comparison() {
        Ok(()) => println!("Model comparison completed!"),
        Err(e) => println!("Model comparison failed: {}", e),
    }
    
    println!("\n{}", "=".repeat(50));
    println!("=== YOLO Inference Benchmark (5 runs each) ===");
    
    println!("\n1. Running GPU (WGPU) inference benchmark:");
    let gpu_start = std::time::Instant::now();
    match run_inference_gpu() {
        Ok(()) => {
            let gpu_time = gpu_start.elapsed();
            println!("GPU (WGPU) benchmark completed in {:.2}s (total including model loading)!", gpu_time.as_secs_f32());
        }
        Err(e) => {
            println!("GPU (WGPU) inference failed: {}", e);
        }
    }
    
    println!("\n2. Running CPU inference benchmark:");
    let cpu_start = std::time::Instant::now();
    match run_inference_cpu() {
        Ok(()) => {
            let cpu_time = cpu_start.elapsed();
            println!("CPU benchmark completed in {:.2}s (total including model loading)!", cpu_time.as_secs_f32());
        }
        Err(e) => {
            println!("CPU inference failed: {}", e);
        }
    }
    
    println!("\n=== Benchmark Summary ===");
    println!("‚úì Ran 5 inference iterations for both GPU and CPU");
    println!("‚úì Statistics include average, minimum, and maximum times");
    println!("‚úì Only pure inference time is measured (excluding setup/teardown)");
    println!("‚úì Both outputs saved for verification");
    println!("\nüí° Backend Compatibility Summary:");
    println!("   ‚úÖ WGPU: Fully compatible with YOLOX models");
    println!("   ‚úÖ NdArray: Stable CPU backend, good for debugging");
    println!("   ‚ùå CUDA (Candle): Limited by pooling operation support in YOLOX");
    println!("   üìù For production: ONNX Runtime or TensorRT recommended");
    println!("   üî¨ Burn framework: Research-focused, evolving ecosystem");
    
    Ok(())
}